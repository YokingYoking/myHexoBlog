---
title:      pytorch入门（二）简单的预测器
date:       2020-03-03
categories:
    - 学习笔记
tags:
    - 深度学习
    - pytorch
    - python
excerpt: 从pytorch入门人工智能。
---

# 简单的预测器

顾名思义，有一定的数据量，分为训练集和测试集（此处暂时不使用validation set），通过训练集训练出一个模型，再将测试集放进去对比其准确度。

## 前期准备

### 1.数据预处理

如星期几，天气等表示某种**类型**的变量，使用独热码给予一个向量。
如星期一到星期天，分别对应1000000、0100000···0000001.

而对于数据有实际意义但是由于衡量单位有所不同的几种变量，我们需要进行**归一化**处理，如温度、湿度等。

（归一化：每个数据减均值后再除以方差）

### 2.划分数据

划分训练集(training set)、测试集(test set)。

并要进行分批(batch)处理，每批相同数量的数据，每批训练完成后计算loss，BP算法，调整权重，以保证网络更加收敛。（在pytorch代码中会实现）

## pytorch代码建立

### 1.构建神经网络

```python
import torch
neu = torch.nn.Sequential(  # 建立一个多部操作的神经网络模型
    torch.nn.Linear(input_size,hidden_size), #  输入的那个节点（为线性的）
    torch.nn.Sigmoid(), #  设置激活函数（activate function）为sigmoid，也就是隐含层内部之间的非线性关系
    torch.nn.Linear(hidden_size,output_size) #  输出的节点，同为线性的
)
neu.parameter() #  返回神经网络的参数（所有权重W、偏置参数）
```

### 2.建立损失函数(loss function)和优化器(optimizer)

```python
cost = torch.nn.MSELoss() #  定义损失函数为均方差，相当于torch.mean((y-y*)^2)
#  cost为一个函数指针
optimizer = torch.optim.SGD(neu.parameter(),lr=0.01) #  创建优化器并使用SGD随机梯度下降算法
                                                     #  lr为学习率，learning rate
```

### 3.对神经网络分批次进行训练

```python
# batch_size = 128
for i in range(2000): #  循环两千次训练
    batch_loss = [] #  以list形式记录每一批的loss
    for start in range(0,len(X),batch_size): #  start是1批数据中的起始下标
        end = start + batch_size if start + batch_size < len(X) else len(X) #  end是1批数据中的结束下标，此处if else判断是针对最后一个batch的情况
        #  X是所有的特征属性数据的数组
        xx = Variable(torch.FloatTensor(X[start:end])) #  得到一个批次中的特征属性数据
        yy = Variable(torch.FloatTensor(Y[start:end])) #  得到一个批次中的目标属性数据
        #  Variable 与 tensor类型已经合并了，这里写是为了方便，写成tensor后加上tensor__grad = true 效果一致
        predict = neu(xx) #  模型预测
        loss = cost(predict,yy) #  计算损失
        optimizer.zero_grad() #  将优化器储存的参数梯度清零，也就是上一次计算留下来的梯度
        loss.backward() # 进行反向传播，计算所有新的梯度值
        optimizer.step() #  优化器向前运行一步（梯度下降）,更新所有参数
        batch__loss.append(loss.data.numpy()) #  将loss压到list中
if i % 100 == 0:
    losses.append(np.mean(batch_loss)) #  每循环100次计算一次loss
    print(i,np.mean(batch_loss))
```

### 4.输入测试集与实际数据相对比

```python

testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)
#  将测试集的10000张图片划分成2500份，每份4张图
correct = 0 #  记录正确个数
total = 0 #  测试集数据总数
for data in testloader:
    images,labels = data #  以图片分类为例
    outputs = neu(Variable(images)) #  放入测试集数据
    #print outputs.data
    predicted = torch.max(outputs.data, 1) #  此处outputs.data是一个batch_size x 10的张量，将每一行的最大的那一列的值和序号各自组成一个一维张量返回，第一个是值的张量，第二个是序号的张量。
    #  具体情况根据实际情况选出预测结果
    total += labels.size(0) #  逐个计数总共多少个测试数据，虽然按理说本来就知道，这一句可省
    correct += (predicted == labels).sum()   #两个一维张量逐行对比，相同的行记为1，不同的行记为0，再利用sum(),求总和，得到相同的个数。
print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))
```

## 总结

在有一定的深度学习基础知识后，pytorch框架中的很多api其实方便了我们的操作，不需要老老实实的一步一步搭建网络，但是对于底层知识一定要扎实，调用api时才知道自己每一步做的什么。
